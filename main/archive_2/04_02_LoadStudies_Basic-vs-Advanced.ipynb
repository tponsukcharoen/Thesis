{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "import patsy #for spline regression\n",
    "import scipy #for non-negative least square \n",
    "import scipy as sp \n",
    "from scipy import stats\n",
    "from scipy.optimize import nnls\n",
    "from numpy.linalg import inv #for matrix and statistics\n",
    "import scipy as sp\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.cluster\n",
    "import sklearn.linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sklearn.cluster\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVR\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify data source \n",
    "[region,zipcode] = ['PJM','08641']\n",
    "#[region,zipcode]  = ['CAISO','92562']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mainDir = 'C:\\Users\\Admin\\Dropbox\\Active\\EnergyProject\\Thesis'\n",
    "dataDir = mainDir + '/data/load/' + region +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if region == 'CAISO':\n",
    "    \n",
    "    #timezone for local time adjustment\n",
    "    localtz = 'US/Pacific'\n",
    "\n",
    "    #specific to CAISO data\n",
    "    TacName = 'Caiso_Totals'#'TAC_NORTH'\n",
    "    \n",
    "    datasource = 'raw/caiso_load_data_2013'\n",
    "    \n",
    "    d01 = pd.read_csv(dataDir + datasource + '/'+ '01.csv')\n",
    "    d01 = d01[d01['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d02 = pd.read_csv(dataDir + datasource +'/'+ '02.csv')\n",
    "    d02 = d02[d02['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d03 = pd.read_csv(dataDir + datasource +'/'+ '03.csv')\n",
    "    d03 = d03[d03['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d04 = pd.read_csv(dataDir + datasource + '/'+ '04.csv')\n",
    "    d04 = d04[d04['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d05 = pd.read_csv(dataDir + datasource + '/'+ '05.csv')\n",
    "    d05 = d05[d05['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d06 = pd.read_csv(dataDir + datasource + '/'+ '06.csv')\n",
    "    d06 = d06[d06['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d07 = pd.read_csv(dataDir + datasource + '/'+ '07.csv')\n",
    "    d07 = d07[d07['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d08 = pd.read_csv(dataDir + datasource + '/'+ '08.csv')\n",
    "    d08 = d08[d08['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d09 = pd.read_csv(dataDir + datasource + '/'+ '09.csv')\n",
    "    d09 = d09[d09['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d10 = pd.read_csv(dataDir + datasource + '/'+ '10.csv')\n",
    "    d10 = d10[d10['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d11 = pd.read_csv(dataDir + datasource + '/'+ '11.csv')\n",
    "    d11 = d11[d11['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d12 = pd.read_csv(dataDir + datasource + '/'+ '12.csv')\n",
    "    d12 = d12[d12['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    data = pd.concat([d01,d02,d03,d04,d05,d06,d07,d08,d09,d10,d11,d12])\n",
    "    \n",
    "    datasource = 'raw/caiso_load_data_2014'\n",
    "    \n",
    "    d01 = pd.read_csv(dataDir + datasource + '/'+ '01.csv')\n",
    "    d01 = d01[d01['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d02 = pd.read_csv(dataDir + datasource + '/'+ '02.csv')\n",
    "    d02 = d02[d02['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d03 = pd.read_csv(dataDir + datasource + '/'+ '03.csv')\n",
    "    d03 = d03[d03['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d04 = pd.read_csv(dataDir + datasource + '/'+ '04.csv')\n",
    "    d04 = d04[d04['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d05 = pd.read_csv(dataDir + datasource + '/'+ '05.csv')\n",
    "    d05 = d05[d05['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d06 = pd.read_csv(dataDir + datasource + '/'+ '06.csv')\n",
    "    d06 = d06[d06['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d07 = pd.read_csv(dataDir + datasource + '/'+ '07.csv')\n",
    "    d07 = d07[d07['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d08 = pd.read_csv(dataDir + datasource + '/'+ '08.csv')\n",
    "    d08 = d08[d08['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d09 = pd.read_csv(dataDir + datasource + '/'+ '09.csv')\n",
    "    d09 = d09[d09['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d10 = pd.read_csv(dataDir + datasource + '/'+ '10.csv')\n",
    "    d10 = d10[d10['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d11 = pd.read_csv(dataDir + datasource + '/'+ '11.csv')\n",
    "    d11 = d11[d11['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d12 = pd.read_csv(dataDir + datasource + '/'+ '12.csv')\n",
    "    d12 = d12[d12['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    data = pd.concat([data,d01,d02,d03,d04,d05,d06,d07,d08,d09,d10,d11,d12])\n",
    "\n",
    "    data['tsLocal'] = data['INTERVALSTARTTIME_GMT'].map(lambda x: datetime.datetime.strptime(x[0:19],\"%Y-%m-%dT%H:%M:%S\").replace(tzinfo=pytz.utc)\\\n",
    "                                .astimezone(pytz.timezone(localtz)).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    data['tsLocal'] = data['tsLocal'].map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    data = data[['tsLocal','MW']].reset_index().drop(['index'],1)\n",
    "    data.columns = ['tsLocal','load']\n",
    "    #delete double readings (can happen due to daylight savings)\n",
    "    data = data.groupby(['tsLocal']).mean().reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** file size (6815616) not 512 + multiple of sector size (512)\n"
     ]
    }
   ],
   "source": [
    "if region == 'PJM':\n",
    "    \n",
    "    datasource = 'raw'\n",
    "    #filename = 'pjm_e_load_data_2012.csv'\n",
    "    filename = '2013-PJM-hourly-loads.xls'\n",
    "    data = pd.read_excel(dataDir + datasource + '/'+filename,sheetname='RTO')\n",
    "    filename = '2014-PJM-hourly-loads.xls'\n",
    "    d = pd.read_excel(dataDir + datasource + '/'+filename,sheetname='RTO')\n",
    "    data = pd.concat([data,d])\n",
    "    \n",
    "    data = data.loc[:,data.columns[0:26]]\n",
    "    #set DAY as index\n",
    "    data = data.set_index('DATE')\n",
    "    #remove unnecessary column 'peak'\n",
    "    data = data.drop('COMP', 1)\n",
    "    #unstack data and rename columns\n",
    "    data = data.unstack().reset_index()\n",
    "    data.columns = ['hour','date','load']\n",
    "\n",
    "    #convert hour and date to timestamp then sort by tsLocal\n",
    "    data['tsLocal'] = data['date'].map(lambda x: datetime.datetime.strptime(x,\"%m/%d/%Y\"))+\\\n",
    "                    data['hour'].map(lambda x: datetime.timedelta(hours=int(str(x)[2:4])))\n",
    "    data = data.sort(['tsLocal'])\n",
    "    #don't need weekday for now\n",
    "    #data['weekday'] = data['tsLocal'].map(lambda x: x.weekday())\n",
    "    #reset index and drop uncesseary index\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(['index','hour','date'], 1)\n",
    "\n",
    "    #delete double readings (can happen due to daylight savings)\n",
    "    data = data.groupby(['tsLocal']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#weekday info\n",
    "data['weekday'] = data['tsLocal'].map(lambda x: x.weekday())\n",
    "#date and hour info\n",
    "#add date and hour, normalized to 1\n",
    "data['d'] = data.tsLocal.map(lambda x: x.timetuple().tm_yday)/365.0\n",
    "data['h'] = data.tsLocal.map(lambda x: x.timetuple().tm_hour)/24.0\n",
    "loaddata = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mainDir = 'C:/Users/umnouyp/Dropbox/Active/EnergyProject/Thesis/PVreadingsStudies/main'\n",
    "#mainDir = 'C:/Users/Tee/Dropbox/Active/EnergyProject/Thesis/PVreadingsStudies/main'\n",
    "mainDir = 'C:\\Users\\Admin\\Dropbox\\Active\\EnergyProject\\Thesis'\n",
    "#we can choose which data to look at.\n",
    "dataDir = mainDir + '/data/solar/' + datasource +'/' + zipcode +'/'\n",
    "outputDir = mainDir + '/output/'+ datasource +'/' + zipcode +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherdatasource = '/data/weather/weathersource/hourly/'\n",
    "if (zipcode == '08641')|(zipcode =='08640'):\n",
    "    weatherdata = pd.read_csv(mainDir+weatherdatasource+'08641_20132014.csv')\n",
    "if (zipcode == '92562')|(zipcode =='92563'):\n",
    "    weatherdata = pd.read_csv(mainDir+weatherdatasource+'92563_20132014.csv')\n",
    "\n",
    "weatherdata['tsLocal'] = weatherdata.timestamp.map(lambda x: x[0:10] + \" \" + x[11:19])\n",
    "#weatherdata.tsLocal = weatherdata.tsLocal.map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "if type(weatherdata.tsLocal[0])==str:\n",
    "    weatherdata['tsLocal'] = weatherdata['tsLocal'].map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "weatherdata.drop(['timestamp','country','postal_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if weatherdata.shape[0]!=len(set(weatherdata['tsLocal'])):\n",
    "    for i in range(weatherdata.shape[0]):\n",
    "        if weatherdata.loc[i,'tsLocal'] == weatherdata.loc[(i+1),'tsLocal']:\n",
    "            print weatherdata.loc[i,'tsLocal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic time series construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify scope\n",
    "StartScopeDate = '2013-01-01'\n",
    "EndScopeDate = '2015-01-01'\n",
    "StartScopeTime =  datetime.datetime.strptime(StartScopeDate,\"%Y-%m-%d\")\n",
    "EndScopeTime =  datetime.datetime.strptime(EndScopeDate,\"%Y-%m-%d\")\n",
    "\n",
    "#Generate data frame from start to end for time series\n",
    "tslist = []\n",
    "nxt = StartScopeTime\n",
    "while nxt < EndScopeTime:\n",
    "    tslist.append(nxt)\n",
    "    nxt += datetime.timedelta(minutes=60) #hour data now\n",
    "        \n",
    "Scope = pd.DataFrame(0, index = np.arange(len(tslist)), columns = ['tsLocal'])\n",
    "Scope['tsLocal'] = tslist\n",
    "\n",
    "#Now we can merge using Scope Table as a backbone of data structure.\n",
    "d = pd.merge(Scope,weatherdata,on=['tsLocal'], how = 'left') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there is no need to do interpolation\n",
    "\n",
    "#Now we can merge our backbone + weather with solar data\n",
    "d = pd.merge(d,loaddata,on=['tsLocal'], how = 'left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weather\n",
    "d['cldCvr-1'] = float('NaN')\n",
    "d['dewPt-1'] = float('NaN')\n",
    "d['feelsLike-1'] = float('NaN')\n",
    "d['precip-1'] = float('NaN')\n",
    "d['relHum-1'] = float('NaN')\n",
    "d['sfcPres-1'] = float('NaN')\n",
    "d['snowfall-1'] = float('NaN')\n",
    "d['spcHum-1'] = float('NaN')\n",
    "d['temp-1'] = float('NaN')\n",
    "d['windSpd-1'] = float('NaN')\n",
    "d['wetBulb-1'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'cldCvr-1'] = list(d['cldCvr'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'dewPt-1'] = list(d['dewPt'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'feelsLike-1'] = list(d['feelsLike'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'precip-1'] = list(d['precip'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'relHum-1'] = list(d['relHum'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'sfcPres-1'] = list(d['sfcPres'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'snowfall-1'] = list(d['snowfall'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'spcHum-1'] = list(d['spcHum'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'temp-1'] = list(d['temp'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'windSpd-1'] = list(d['windSpd'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'wetBulb-1'] = list(d['wetBulb'].iloc[0:(d.shape[0]-1)])\n",
    "\n",
    "#normalize weather value\n",
    "d['cldCvr-1'] = (d['cldCvr-1']-np.min(d['cldCvr-1']))/(np.max(d['cldCvr-1'])-np.min(d['cldCvr-1']))\n",
    "d['dewPt-1'] = (d['dewPt-1']-np.min(d['dewPt-1']))/(np.max(d['dewPt-1'])-np.min(d['dewPt-1']))\n",
    "d['feelsLike-1'] = (d['feelsLike-1']-np.min(d['feelsLike-1']))/(np.max(d['feelsLike-1'])-np.min(d['feelsLike-1']))\n",
    "d['precip-1'] = (d['precip-1']-np.min(d['precip-1']))/(np.max(d['precip-1'])-np.min(d['precip-1']))\n",
    "d['relHum-1'] = (d['relHum-1']-np.min(d['relHum-1']))/(np.max(d['relHum-1'])-np.min(d['relHum-1']))\n",
    "d['sfcPres-1'] = (d['sfcPres-1']-np.min(d['sfcPres-1']))/(np.max(d['sfcPres-1'])-np.min(d['sfcPres-1']))\n",
    "d['snowfall-1'] = (d['snowfall-1']-np.min(d['snowfall-1']))/(np.max(d['snowfall-1'])-np.min(d['snowfall-1']))\n",
    "d['spcHum-1'] = (d['spcHum-1']-np.min(d['spcHum-1']))/(np.max(d['spcHum-1'])-np.min(d['spcHum-1']))\n",
    "d['temp-1'] = (d['temp-1']-np.min(d['temp-1']))/(np.max(d['temp-1'])-np.min(d['temp-1']))\n",
    "d['windSpd-1'] = (d['windSpd-1']-np.min(d['windSpd-1']))/(np.max(d['windSpd-1'])-np.min(d['windSpd-1']))\n",
    "d['wetBulb-1'] = (d['wetBulb-1']-np.min(d['wetBulb-1']))/(np.max(d['wetBulb-1'])-np.min(d['wetBulb-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#timeseries of previous timestamps\n",
    "d['load-1d'] = float('NaN') #previous day\n",
    "d['load-2d'] = float('NaN') #previous day\n",
    "d['load-3d'] = float('NaN') #previous day\n",
    "d['load-4d'] = float('NaN') #previous day\n",
    "d['load-5d'] = float('NaN') #previous day\n",
    "d['load-6d'] = float('NaN') #previous day\n",
    "d['load-1w'] = float('NaN') #previous 7 days\n",
    "d['load-1h'] = float('NaN') #previous hr\n",
    "d['load-1d-1h'] = float('NaN')\n",
    "d['load-1w-1h'] = float('NaN')\n",
    "d['load-2h'] = float('NaN') #previous hr\n",
    "d['load-1d-2h'] = float('NaN') #previous hr\n",
    "d['load-1w-2h'] = float('NaN') #previous hr\n",
    "\n",
    "#24 hour shift = 24 shift in index.\n",
    "\n",
    "d.loc[24:d.shape[0],'load-1d'] = list(d['load'].iloc[0:(d.shape[0]-24)])\n",
    "d.loc[(24*2):d.shape[0],'load-2d'] = list(d['load'].iloc[0:(d.shape[0]-(24*2))])\n",
    "d.loc[(24*3):d.shape[0],'load-3d'] = list(d['load'].iloc[0:(d.shape[0]-(24*3))])\n",
    "d.loc[(24*4):d.shape[0],'load-4d'] = list(d['load'].iloc[0:(d.shape[0]-(24*4))])\n",
    "d.loc[(24*5):d.shape[0],'load-5d'] = list(d['load'].iloc[0:(d.shape[0]-(24*5))])\n",
    "d.loc[(24*6):d.shape[0],'load-6d'] = list(d['load'].iloc[0:(d.shape[0]-(24*6))])\n",
    "d.loc[(24*7):d.shape[0],'load-1w'] = list(d['load'].iloc[0:(d.shape[0]-(24*7))])\n",
    "\n",
    "d.loc[1:d.shape[0],'load-1h'] = list(d['load'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[25:d.shape[0],'load-1d-1h'] = list(d['load'].iloc[0:(d.shape[0]-25)])\n",
    "d.loc[(24*7+1):d.shape[0],'load-1w-1h'] = list(d['load'].iloc[0:(d.shape[0]-(24*7+1))])\n",
    "d.loc[2:d.shape[0],'load-2h'] = list(d['load'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[26:d.shape[0],'load-1d-2h'] = list(d['load'].iloc[0:(d.shape[0]-26)])\n",
    "d.loc[(24*7+2):d.shape[0],'load-1w-2h'] = list(d['load'].iloc[0:(d.shape[0]-(24*7+2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8760, 40), (8760, 40))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['tsLocal'].map(lambda x: x.year==2013)].shape,d[d['tsLocal'].map(lambda x: x.year==2014)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adjust date correctly\n",
    "datetime.date(2013,1,1).weekday(),datetime.date(2014,1,1).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tsLocal</th>\n",
       "      <th>cldCvr</th>\n",
       "      <th>dewPt</th>\n",
       "      <th>feelsLike</th>\n",
       "      <th>precip</th>\n",
       "      <th>relHum</th>\n",
       "      <th>sfcPres</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>spcHum</th>\n",
       "      <th>temp</th>\n",
       "      <th>...</th>\n",
       "      <th>load-4d</th>\n",
       "      <th>load-5d</th>\n",
       "      <th>load-6d</th>\n",
       "      <th>load-1w</th>\n",
       "      <th>load-1h</th>\n",
       "      <th>load-1d-1h</th>\n",
       "      <th>load-1w-1h</th>\n",
       "      <th>load-2h</th>\n",
       "      <th>load-1d-2h</th>\n",
       "      <th>load-1w-2h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8760</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td> 15</td>\n",
       "      <td> 12.5</td>\n",
       "      <td> 19.3</td>\n",
       "      <td> 0</td>\n",
       "      <td> 56.2</td>\n",
       "      <td> 1022.8</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1.64</td>\n",
       "      <td> 26</td>\n",
       "      <td>...</td>\n",
       "      <td> 90592.079</td>\n",
       "      <td> 94092.666</td>\n",
       "      <td> 90176.256</td>\n",
       "      <td> 96348.839</td>\n",
       "      <td> 97502.767</td>\n",
       "      <td> 99773.947</td>\n",
       "      <td> 99644.327</td>\n",
       "      <td> 100683.011</td>\n",
       "      <td> 104914.285</td>\n",
       "      <td> 101478.199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tsLocal  cldCvr  dewPt  feelsLike  precip  relHum  sfcPres  snowfall  \\\n",
       "8760 2014-01-01      15   12.5       19.3       0    56.2   1022.8         0   \n",
       "\n",
       "      spcHum  temp     ...        load-4d    load-5d    load-6d    load-1w  \\\n",
       "8760    1.64    26     ...      90592.079  94092.666  90176.256  96348.839   \n",
       "\n",
       "        load-1h  load-1d-1h  load-1w-1h     load-2h  load-1d-2h  load-1w-2h  \n",
       "8760  97502.767   99773.947   99644.327  100683.011  104914.285  101478.199  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d.tsLocal.map(lambda x: x.date() == datetime.date(2014,1,1))][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['load-1y'] = float('NaN')\n",
    "#do one year shift. With weekday correction\n",
    "d.loc[(8759):d.shape[0],'load-1y'] = list(d['load'].iloc[24:(d.shape[0]-(8759-24))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: to make a reference, use previous day first. For Monday and Saturday, use average of previous week and previous year with week alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['load_ref'] = d['load-1d']*((d['weekday']!=0)&(d['weekday']!=5))+\\\n",
    "                    d['load-1w']*(((d['weekday']==0)|(d['weekday']==5)))\n",
    "                    #0.5*(d['load-1w']+d['load-1y'])*(((d['weekday']==0)|(d['weekday']==5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extra modification to make it smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ref... not use anymore \n",
    "#d['load_ref+1'] = float('NaN')\n",
    "#d['load_ref-1'] = float('NaN')\n",
    "#d.loc[1:d.shape[0],'load_ref-1'] = list(d['load_ref'].iloc[0:(d.shape[0]-1)])\n",
    "#d.loc[0:(d.shape[0]-2),'load_ref+1'] = list(d['load_ref'].iloc[1:(d.shape[0])])\n",
    "#d['load_ref'] = (d['load_ref']+d['load_ref-1']+d['load_ref+1'])/3.0\n",
    "\n",
    "#for day\n",
    "d['load-1d+1'] = float('NaN')\n",
    "d['load-1d-1'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'load-1d-1'] = list(d['load-1d'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[0:(d.shape[0]-2),'load-1d+1'] = list(d['load-1d'].iloc[1:(d.shape[0])])\n",
    "d['load-1d_smooth'] = (d['load-1d']+d['load-1d-1']+d['load-1d+1'])/3.0\n",
    "\n",
    "#for week\n",
    "d['load-1w+1'] = float('NaN')\n",
    "d['load-1w-1'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'load-1w-1'] = list(d['load-1w'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[0:(d.shape[0]-2),'load-1w+1'] = list(d['load-1w'].iloc[1:(d.shape[0])])\n",
    "d['load-1w_smooth'] = (d['load-1w']+d['load-1w-1']+d['load-1w+1'])/3.0\n",
    "\n",
    "'''\n",
    "#Define ratio r\n",
    "d['r'] = d['load']/d['load_ref']\n",
    "d['r-1'] = float('NaN')\n",
    "d['r-2'] = float('NaN')\n",
    "d['r-3'] = float('NaN')\n",
    "d['r-4'] = float('NaN')\n",
    "d['r-5'] = float('NaN')\n",
    "d['r-6'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'r-1'] = list(d['r'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[2:d.shape[0],'r-2'] = list(d['r'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[3:d.shape[0],'r-3'] = list(d['r'].iloc[0:(d.shape[0]-3)])\n",
    "d.loc[4:d.shape[0],'r-4'] = list(d['r'].iloc[0:(d.shape[0]-4)])\n",
    "d.loc[5:d.shape[0],'r-5'] = list(d['r'].iloc[0:(d.shape[0]-5)])\n",
    "d.loc[6:d.shape[0],'r-6'] = list(d['r'].iloc[0:(d.shape[0]-6)])\n",
    "'''\n",
    "\n",
    "#Define finer ratio\n",
    "d['rd'] = d['load']/d['load-1d']\n",
    "d['rw'] = d['load']/d['load-1w']\n",
    "#d['ry'] = d['load']/d['load-1y']\n",
    "d['rd-1'] = float('NaN')\n",
    "d['rw-1'] = float('NaN')\n",
    "#d['ry-1'] = float('NaN')\n",
    "d['rd-2'] = float('NaN')\n",
    "d['rw-2'] = float('NaN')\n",
    "#d['ry-2'] = float('NaN')\n",
    "d['rd-3'] = float('NaN')\n",
    "d['rw-3'] = float('NaN')\n",
    "#d['ry-2'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'rd-1'] = list(d['rd'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'rw-1'] = list(d['rw'].iloc[0:(d.shape[0]-1)])\n",
    "#d.loc[1:d.shape[0],'ry-1'] = list(d['ry'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[2:d.shape[0],'rd-2'] = list(d['rd'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[2:d.shape[0],'rw-2'] = list(d['rw'].iloc[0:(d.shape[0]-2)])\n",
    "#d.loc[2:d.shape[0],'ry-2'] = list(d['ry'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[3:d.shape[0],'rd-3'] = list(d['rd'].iloc[0:(d.shape[0]-3)])\n",
    "d.loc[3:d.shape[0],'rw-3'] = list(d['rw'].iloc[0:(d.shape[0]-3)])\n",
    "#d.loc[3:d.shape[0],'ry-3'] = list(d['ry'].iloc[0:(d.shape[0]-3)])\n",
    "\n",
    "#feed this back to data\n",
    "Data = d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1-hr rolling prediction: Naive model, load and 'r' investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Data\n",
    "#Truly random\n",
    "#l = np.random.permutation(d.shape[0])\n",
    "#d = d.iloc[l].reset_index(drop=True)\n",
    "#offset = int(d.shape[0] * 0.8)\n",
    "\n",
    "#Assign specific period\n",
    "StartDate = datetime.date(2014,10,1)\n",
    "offset = d[d.tsLocal.map(lambda x: x.date()) <StartDate].shape[0]\n",
    "\n",
    "Data_train = d[:offset]\n",
    "Data_test = d[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120847.897, 86464.576358858176)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Data_test['load']), np.mean(Data_test['load'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Reference: Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = Data_train\n",
    "d_test = Data_test\n",
    "feature_names = ['rd-1','rw-1']\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[606.91442847187125, 5133.1581509796379]\n"
     ]
    }
   ],
   "source": [
    "# with reference with previous day + week\n",
    "d_test['load_predict']=float('NaN')\n",
    "d_test['load_predict']=0.5*(d_test['rd-1']*d_test['load-1d']+d_test['rw-1']*d_test['load-1w'])\n",
    "\n",
    "l_0 = d_test['load']-d_test['load_predict']\n",
    "[mean_persistence,max_persistence] = [np.mean(np.absolute(l_0)),np.max(np.absolute(l_0))]\n",
    "\n",
    "print [mean_persistence,max_persistence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Auto-regression 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[608.55703863602821, 5132.273939469349]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "\n",
    "\n",
    "l_1 = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_AR1,max_AR1] = [np.mean(np.absolute(l_1)),np.max(np.absolute(l_1))]\n",
    "\n",
    "print [mean_AR1,max_AR1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[605.60835578653348, 5039.5821681385423]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "for i in range(0,7):\n",
    "    Index_train = d_train[d_train['tsLocal'].map(lambda x: ((x.weekday()==i)))].index\n",
    "    Index_test = d_test[d_test['tsLocal'].map(lambda x: ((x.weekday()==i)))].index\n",
    "    \n",
    "    feature_names = ['rw-1']\n",
    "    X_train, y_train = d_train.loc[Index_train,feature_names], d_train.loc[Index_train,'rw']\n",
    "    X_test, y_test = d_test.loc[Index_test,feature_names], d_test.loc[Index_test,'rw']\n",
    "    mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    d_test.loc[Index_test,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[Index_test,'load-1w']\n",
    "    \n",
    "    feature_names = ['rd-1']\n",
    "    X_train, y_train = d_train.loc[Index_train,feature_names], d_train.loc[Index_train,'rd']\n",
    "    X_test, y_test = d_test.loc[Index_test,feature_names], d_test.loc[Index_test,'rd']\n",
    "    mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    d_test.loc[Index_test,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[Index_test,'load-1d']\n",
    "\n",
    "\n",
    "l_1s = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_AR1,max_AR1] = [np.mean(np.absolute(l_1s)),np.max(np.absolute(l_1s))]\n",
    "\n",
    "print [mean_AR1,max_AR1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Auto-regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[415.61181117048176, 5729.0693479674519]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "\n",
    "\n",
    "l_2 = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_AR2,max_AR2] = [np.mean(np.absolute(l_2)),np.max(np.absolute(l_2))]\n",
    "\n",
    "print [mean_AR2,max_AR2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410.43136434896189, 5757.4896555972227]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "for i in range(0,7):\n",
    "    Index_train = d_train[d_train['tsLocal'].map(lambda x: ((x.weekday()==i)))].index\n",
    "    Index_test = d_test[d_test['tsLocal'].map(lambda x: ((x.weekday()==i)))].index\n",
    "    \n",
    "    feature_names = ['rw-1','rw-2']\n",
    "    X_train, y_train = d_train.loc[Index_train,feature_names], d_train.loc[Index_train,'rw']\n",
    "    X_test, y_test = d_test.loc[Index_test,feature_names], d_test.loc[Index_test,'rw']\n",
    "    mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    d_test.loc[Index_test,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[Index_test,'load-1w']\n",
    "\n",
    "    feature_names = ['rd-1','rd-2']\n",
    "    X_train, y_train = d_train.loc[Index_train,feature_names], d_train.loc[Index_train,'rd']\n",
    "    X_test, y_test = d_test.loc[Index_test,feature_names], d_test.loc[Index_test,'rd']\n",
    "    mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    d_test.loc[Index_test,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[Index_test,'load-1d']\n",
    "\n",
    "l_2s = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_AR2,max_AR2] = [np.mean(np.absolute(l_2s)),np.max(np.absolute(l_2s))]\n",
    "\n",
    "print [mean_AR2,max_AR2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Auto-regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[412.60752976039618, 6804.4477671432251]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','rw-3','rd-3']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','rw-3']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','rd-3']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "\n",
    "\n",
    "l_3 = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_AR2,max_AR2] = [np.mean(np.absolute(l_3)),np.max(np.absolute(l_3))]\n",
    "\n",
    "print [mean_AR2,max_AR2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406.01116201828273, 6688.253955431428]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','rw-3','rd-3']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "for i in range(0,7):\n",
    "    Index_train = d_train[d_train['tsLocal'].map(lambda x: ((x.weekday()==i)))].index\n",
    "    Index_test = d_test[d_test['tsLocal'].map(lambda x: ((x.weekday()==i)))].index\n",
    "    \n",
    "    feature_names = ['rw-1','rw-2','rw-3']\n",
    "    X_train, y_train = d_train.loc[Index_train,feature_names], d_train.loc[Index_train,'rw']\n",
    "    X_test, y_test = d_test.loc[Index_test,feature_names], d_test.loc[Index_test,'rw']\n",
    "    mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    d_test.loc[Index_test,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[Index_test,'load-1w']\n",
    "\n",
    "    feature_names = ['rd-1','rd-2','rd-3']\n",
    "    X_train, y_train = d_train.loc[Index_train,feature_names], d_train.loc[Index_train,'rd']\n",
    "    X_test, y_test = d_test.loc[Index_test,feature_names], d_test.loc[Index_test,'rd']\n",
    "    mod = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    d_test.loc[Index_test,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[Index_test,'load-1d']\n",
    "\n",
    "l_3s = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_AR2,max_AR2] = [np.mean(np.absolute(l_3s)),np.max(np.absolute(l_3s))]\n",
    "\n",
    "print [mean_AR2,max_AR2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex model: SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameter for SVR\n",
    "from sklearn.svm import SVR\n",
    "C = 2.0\n",
    "epsilon=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[609.81693985082154, 5157.5662990037381]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set1,max_set1] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set1,max_set1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning\n",
    "\n",
    "(C=1.0, epsilon=0.1) = [1480.2323518060184, 8559.8547304030071]\n",
    "\n",
    "(C=1.0, epsilon=0.05) = [932.15703092060403, 7013.5933323490608]\n",
    "\n",
    "(C=1.0, epsilon=0.01) = [915.29783292919194, 6971.3207346325507]\n",
    "\n",
    "(C=1.0, epsilon=0.005) = [912.89754875693814, 6935.2838413140271]\n",
    "\n",
    "(C=1.0, epsilon=0.001) = [912.369922908896, 6919.9601870007464]\n",
    "\n",
    "(C=1.0, epsilon=0.0005) = [912.39092358324729, 6917.8635301878967]\n",
    "\n",
    "(C=1.0, epsilon=0.0001) = [912.94493019560116, 6930.3295250666561]\n",
    "\n",
    "(C=2.0, epsilon=0.0005) = [912.06752424337424, 6908.2205535543326]\n",
    "\n",
    "(C=5.0, epsilon=0.0005) = [912.30769907567321, 6909.4569672284269]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[413.81659610998167, 5703.6231565542403]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set2,max_set2] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set2,max_set2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[611.9804346901193, 5033.252334863646]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','d','h']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set3,max_set3] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set3,max_set3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[411.82212750981085, 5673.055868859723]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','d','h']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set4,max_set4] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set4,max_set4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[623.20768399029555, 6699.8668418547604]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set5,max_set5] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set5,max_set5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[418.73167749911323, 5513.9618747321074]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set6,max_set6] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set6,max_set6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex model: GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Gradient Boosting Regression\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.1, 'loss': 'lad'}\n",
    "mod = ensemble.GradientBoostingRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[615.52967452746748, 5110.8343581610243]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set1,max_set1] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set1,max_set1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416.96735072715342, 4525.0665360255225]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set2,max_set2] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set2,max_set2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[585.82806851205612, 5154.3960097080999]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','d','h']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set3,max_set3] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set3,max_set3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[397.76351695116631, 4383.7841217015521]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','d','h']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set4,max_set4] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set4,max_set4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[588.84203837056452, 5199.4528634950111]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set5,max_set5] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set5,max_set5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[415.74295336753255, 4364.0688217284187]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','d','h']\n",
    "feature_names = feature_names +['cldCvr-1','dewPt-1','feelsLike-1','precip-1','relHum-1','sfcPres-1','snowfall-1',\n",
    "                        'spcHum-1','temp-1','windSpd-1','wetBulb-1']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "[mean_set6,max_set6] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set6,max_set6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tuning\n",
    "\n",
    "(n_estimators=1000, max_depth = 4, min_sample_split = 1, learning_rate = 0.01) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=1000, max_depth = 3, min_sample_split = 1, learning_rate = 0.01) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=1000, max_depth = 5, min_sample_split = 1, learning_rate = 0.01) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=500, max_depth = 4, min_sample_split = 1, learning_rate = 0.01) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=2000, max_depth = 4, min_sample_split = 1, learning_rate = 0.01) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=1000, max_depth = 4, min_sample_split = 2, learning_rate = 0.01) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=1000, max_depth = 4, min_sample_split = 1, learning_rate = 0.05) = [718.14909441670454, 8112.0216666666674]\n",
    "\n",
    "(n_estimators=1000, max_depth = 4, min_sample_split = 1, learning_rate = 0.1) = [718.14909441670454, 8112.0216666666674]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we conclude that SVR is a good model. However, the model has consistent error in a certain date and time. This is because even though we take different reference curves, different day of week seems to have effect. In addition, load reference with last year data is actually bad. So we change reference curve to only yesterday, last week, and treat each day of week individually. We also need to aware to ratio cross-over between days. The best way to do is to use rd and rw separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[411.82212750981074, 5673.055868859723]\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','d','h']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','d','h']\n",
    "\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_w_1'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w_2'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','d','h']\n",
    "\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod = SVR(C=C, epsilon=epsilon).fit(X_train,y_train)\n",
    "d_test.loc[:,'load_predict_d_1'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d_2'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "        \n",
    "l = d_test['load']-0.25*(d_test['load_predict_d_1']+d_test['load_predict_w_1']+d_test['load_predict_d_2']+d_test['load_predict_w_2'])\n",
    "[mean_set7,max_set7] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_set7,max_set7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
