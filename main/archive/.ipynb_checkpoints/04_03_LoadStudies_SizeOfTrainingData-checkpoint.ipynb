{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "import patsy #for spline regression\n",
    "import scipy #for non-negative least square \n",
    "import scipy as sp \n",
    "from scipy import stats\n",
    "from scipy.optimize import nnls\n",
    "from numpy.linalg import inv #for matrix and statistics\n",
    "import scipy as sp\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.cluster\n",
    "import sklearn.linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sklearn.cluster\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVR\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify data source \n",
    "\n",
    "#[region,zipcode]  = ['CAISO','92562']\n",
    "[region,zipcode] = ['PJM','08641']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mainDir = 'C:\\Users\\Admin\\Dropbox\\Active\\EnergyProject\\Thesis'\n",
    "dataDir = mainDir + '/data/load/' + region +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if region == 'CAISO':\n",
    "    \n",
    "    #timezone for local time adjustment\n",
    "    localtz = 'US/Pacific'\n",
    "\n",
    "    #specific to CAISO data\n",
    "    TacName = 'Caiso_Totals'#'TAC_NORTH'\n",
    "    \n",
    "    datasource = 'raw/caiso_load_data_2013'\n",
    "    \n",
    "    d01 = pd.read_csv(dataDir + datasource + '/'+ '01.csv')\n",
    "    d01 = d01[d01['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d02 = pd.read_csv(dataDir + datasource +'/'+ '02.csv')\n",
    "    d02 = d02[d02['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d03 = pd.read_csv(dataDir + datasource +'/'+ '03.csv')\n",
    "    d03 = d03[d03['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d04 = pd.read_csv(dataDir + datasource + '/'+ '04.csv')\n",
    "    d04 = d04[d04['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d05 = pd.read_csv(dataDir + datasource + '/'+ '05.csv')\n",
    "    d05 = d05[d05['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d06 = pd.read_csv(dataDir + datasource + '/'+ '06.csv')\n",
    "    d06 = d06[d06['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d07 = pd.read_csv(dataDir + datasource + '/'+ '07.csv')\n",
    "    d07 = d07[d07['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d08 = pd.read_csv(dataDir + datasource + '/'+ '08.csv')\n",
    "    d08 = d08[d08['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d09 = pd.read_csv(dataDir + datasource + '/'+ '09.csv')\n",
    "    d09 = d09[d09['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d10 = pd.read_csv(dataDir + datasource + '/'+ '10.csv')\n",
    "    d10 = d10[d10['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d11 = pd.read_csv(dataDir + datasource + '/'+ '11.csv')\n",
    "    d11 = d11[d11['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d12 = pd.read_csv(dataDir + datasource + '/'+ '12.csv')\n",
    "    d12 = d12[d12['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    data = pd.concat([d01,d02,d03,d04,d05,d06,d07,d08,d09,d10,d11,d12])\n",
    "    \n",
    "    datasource = 'raw/caiso_load_data_2014'\n",
    "    \n",
    "    d01 = pd.read_csv(dataDir + datasource + '/'+ '01.csv')\n",
    "    d01 = d01[d01['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d02 = pd.read_csv(dataDir + datasource + '/'+ '02.csv')\n",
    "    d02 = d02[d02['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d03 = pd.read_csv(dataDir + datasource + '/'+ '03.csv')\n",
    "    d03 = d03[d03['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d04 = pd.read_csv(dataDir + datasource + '/'+ '04.csv')\n",
    "    d04 = d04[d04['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d05 = pd.read_csv(dataDir + datasource + '/'+ '05.csv')\n",
    "    d05 = d05[d05['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d06 = pd.read_csv(dataDir + datasource + '/'+ '06.csv')\n",
    "    d06 = d06[d06['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d07 = pd.read_csv(dataDir + datasource + '/'+ '07.csv')\n",
    "    d07 = d07[d07['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d08 = pd.read_csv(dataDir + datasource + '/'+ '08.csv')\n",
    "    d08 = d08[d08['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d09 = pd.read_csv(dataDir + datasource + '/'+ '09.csv')\n",
    "    d09 = d09[d09['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d10 = pd.read_csv(dataDir + datasource + '/'+ '10.csv')\n",
    "    d10 = d10[d10['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d11 = pd.read_csv(dataDir + datasource + '/'+ '11.csv')\n",
    "    d11 = d11[d11['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    d12 = pd.read_csv(dataDir + datasource + '/'+ '12.csv')\n",
    "    d12 = d12[d12['TAC_ZONE_NAME']==TacName].sort(columns='INTERVALSTARTTIME_GMT')\n",
    "    data = pd.concat([data,d01,d02,d03,d04,d05,d06,d07,d08,d09,d10,d11,d12])\n",
    "\n",
    "    data['tsLocal'] = data['INTERVALSTARTTIME_GMT'].map(lambda x: datetime.datetime.strptime(x[0:19],\"%Y-%m-%dT%H:%M:%S\").replace(tzinfo=pytz.utc)\\\n",
    "                                .astimezone(pytz.timezone(localtz)).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    data['tsLocal'] = data['tsLocal'].map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    data = data[['tsLocal','MW']].reset_index().drop(['index'],1)\n",
    "    data.columns = ['tsLocal','load']\n",
    "    #delete double readings (can happen due to daylight savings)\n",
    "    data = data.groupby(['tsLocal']).mean().reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** file size (6815616) not 512 + multiple of sector size (512)\n"
     ]
    }
   ],
   "source": [
    "if region == 'PJM':\n",
    "    \n",
    "    datasource = 'raw'\n",
    "    #filename = 'pjm_e_load_data_2012.csv'\n",
    "    filename = '2013-PJM-hourly-loads.xls'\n",
    "    data = pd.read_excel(dataDir + datasource + '/'+filename,sheetname='RTO')\n",
    "    filename = '2014-PJM-hourly-loads.xls'\n",
    "    d = pd.read_excel(dataDir + datasource + '/'+filename,sheetname='RTO')\n",
    "    data = pd.concat([data,d])\n",
    "    \n",
    "    data = data.loc[:,data.columns[0:26]]\n",
    "    #set DAY as index\n",
    "    data = data.set_index('DATE')\n",
    "    #remove unnecessary column 'peak'\n",
    "    data = data.drop('COMP', 1)\n",
    "    #unstack data and rename columns\n",
    "    data = data.unstack().reset_index()\n",
    "    data.columns = ['hour','date','load']\n",
    "\n",
    "    #convert hour and date to timestamp then sort by tsLocal\n",
    "    data['tsLocal'] = data['date'].map(lambda x: datetime.datetime.strptime(x,\"%m/%d/%Y\"))+\\\n",
    "                    data['hour'].map(lambda x: datetime.timedelta(hours=int(str(x)[2:4])))\n",
    "    data = data.sort(['tsLocal'])\n",
    "    #don't need weekday for now\n",
    "    #data['weekday'] = data['tsLocal'].map(lambda x: x.weekday())\n",
    "    #reset index and drop uncesseary index\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(['index','hour','date'], 1)\n",
    "\n",
    "    #delete double readings (can happen due to daylight savings)\n",
    "    data = data.groupby(['tsLocal']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#weekday info\n",
    "data['weekday'] = data['tsLocal'].map(lambda x: x.weekday())\n",
    "#date and hour info\n",
    "#add date and hour, normalized to 1\n",
    "data['d'] = data.tsLocal.map(lambda x: x.timetuple().tm_yday)/365.0\n",
    "data['h'] = data.tsLocal.map(lambda x: x.timetuple().tm_hour)/24.0\n",
    "loaddata = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mainDir = 'C:/Users/umnouyp/Dropbox/Active/EnergyProject/Thesis/PVreadingsStudies/main'\n",
    "#mainDir = 'C:/Users/Tee/Dropbox/Active/EnergyProject/Thesis/PVreadingsStudies/main'\n",
    "mainDir = 'C:\\Users\\Admin\\Dropbox\\Active\\EnergyProject\\Thesis\\PVreadingsStudies\\main'\n",
    "#we can choose which data to look at.\n",
    "dataDir = mainDir + '/data/' + datasource +'/' + zipcode +'/'\n",
    "outputDir = mainDir + '/output/'+ datasource +'/' + zipcode +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherdatasource = '/data/Weather/weathersource/hourly/'\n",
    "if (zipcode == '08641')|(zipcode =='08640'):\n",
    "    weatherdata = pd.read_csv(mainDir+weatherdatasource+'08641_20132014.csv')\n",
    "if (zipcode == '92562')|(zipcode =='92563'):\n",
    "    weatherdata = pd.read_csv(mainDir+weatherdatasource+'92563_20132014.csv')\n",
    "\n",
    "weatherdata['tsLocal'] = weatherdata.timestamp.map(lambda x: x[0:10] + \" \" + x[11:19])\n",
    "#weatherdata.tsLocal = weatherdata.tsLocal.map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "if type(weatherdata.tsLocal[0])==str:\n",
    "    weatherdata['tsLocal'] = weatherdata['tsLocal'].map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "weatherdata.drop(['timestamp','country','postal_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if weatherdata.shape[0]!=len(set(weatherdata['tsLocal'])):\n",
    "    for i in range(weatherdata.shape[0]):\n",
    "        if weatherdata.loc[i,'tsLocal'] == weatherdata.loc[(i+1),'tsLocal']:\n",
    "            print weatherdata.loc[i,'tsLocal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic time series construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify scope\n",
    "StartScopeDate = '2013-01-01'\n",
    "EndScopeDate = '2015-01-01'\n",
    "StartScopeTime =  datetime.datetime.strptime(StartScopeDate,\"%Y-%m-%d\")\n",
    "EndScopeTime =  datetime.datetime.strptime(EndScopeDate,\"%Y-%m-%d\")\n",
    "\n",
    "#Generate data frame from start to end for time series\n",
    "tslist = []\n",
    "nxt = StartScopeTime\n",
    "while nxt < EndScopeTime:\n",
    "    tslist.append(nxt)\n",
    "    nxt += datetime.timedelta(minutes=60) #hour data now\n",
    "        \n",
    "Scope = pd.DataFrame(0, index = np.arange(len(tslist)), columns = ['tsLocal'])\n",
    "Scope['tsLocal'] = tslist\n",
    "\n",
    "#Now we can merge using Scope Table as a backbone of data structure.\n",
    "d = pd.merge(Scope,weatherdata,on=['tsLocal'], how = 'left') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there is no need to do interpolation\n",
    "\n",
    "#Now we can merge our backbone + weather with solar data\n",
    "d = pd.merge(d,loaddata,on=['tsLocal'], how = 'left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weather\n",
    "d['cldCvr-1'] = float('NaN')\n",
    "d['dewPt-1'] = float('NaN')\n",
    "d['feelsLike-1'] = float('NaN')\n",
    "d['precip-1'] = float('NaN')\n",
    "d['relHum-1'] = float('NaN')\n",
    "d['sfcPres-1'] = float('NaN')\n",
    "d['snowfall-1'] = float('NaN')\n",
    "d['spcHum-1'] = float('NaN')\n",
    "d['temp-1'] = float('NaN')\n",
    "d['windSpd-1'] = float('NaN')\n",
    "d['wetBulb-1'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'cldCvr-1'] = list(d['cldCvr'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'dewPt-1'] = list(d['dewPt'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'feelsLike-1'] = list(d['feelsLike'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'precip-1'] = list(d['precip'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'relHum-1'] = list(d['relHum'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'sfcPres-1'] = list(d['sfcPres'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'snowfall-1'] = list(d['snowfall'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'spcHum-1'] = list(d['spcHum'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'temp-1'] = list(d['temp'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'windSpd-1'] = list(d['windSpd'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'wetBulb-1'] = list(d['wetBulb'].iloc[0:(d.shape[0]-1)])\n",
    "\n",
    "#normalize weather value\n",
    "d['cldCvr-1'] = (d['cldCvr-1']-np.min(d['cldCvr-1']))/(np.max(d['cldCvr-1'])-np.min(d['cldCvr-1']))\n",
    "d['dewPt-1'] = (d['dewPt-1']-np.min(d['dewPt-1']))/(np.max(d['dewPt-1'])-np.min(d['dewPt-1']))\n",
    "d['feelsLike-1'] = (d['feelsLike-1']-np.min(d['feelsLike-1']))/(np.max(d['feelsLike-1'])-np.min(d['feelsLike-1']))\n",
    "d['precip-1'] = (d['precip-1']-np.min(d['precip-1']))/(np.max(d['precip-1'])-np.min(d['precip-1']))\n",
    "d['relHum-1'] = (d['relHum-1']-np.min(d['relHum-1']))/(np.max(d['relHum-1'])-np.min(d['relHum-1']))\n",
    "d['sfcPres-1'] = (d['sfcPres-1']-np.min(d['sfcPres-1']))/(np.max(d['sfcPres-1'])-np.min(d['sfcPres-1']))\n",
    "d['snowfall-1'] = (d['snowfall-1']-np.min(d['snowfall-1']))/(np.max(d['snowfall-1'])-np.min(d['snowfall-1']))\n",
    "d['spcHum-1'] = (d['spcHum-1']-np.min(d['spcHum-1']))/(np.max(d['spcHum-1'])-np.min(d['spcHum-1']))\n",
    "d['temp-1'] = (d['temp-1']-np.min(d['temp-1']))/(np.max(d['temp-1'])-np.min(d['temp-1']))\n",
    "d['windSpd-1'] = (d['windSpd-1']-np.min(d['windSpd-1']))/(np.max(d['windSpd-1'])-np.min(d['windSpd-1']))\n",
    "d['wetBulb-1'] = (d['wetBulb-1']-np.min(d['wetBulb-1']))/(np.max(d['wetBulb-1'])-np.min(d['wetBulb-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#timeseries of previous timestamps\n",
    "d['load-1d'] = float('NaN') #previous day\n",
    "d['load-2d'] = float('NaN') #previous day\n",
    "d['load-3d'] = float('NaN') #previous day\n",
    "d['load-4d'] = float('NaN') #previous day\n",
    "d['load-5d'] = float('NaN') #previous day\n",
    "d['load-6d'] = float('NaN') #previous day\n",
    "d['load-1w'] = float('NaN') #previous 7 days\n",
    "d['load-1h'] = float('NaN') #previous hr\n",
    "d['load-1d-1h'] = float('NaN')\n",
    "d['load-1w-1h'] = float('NaN')\n",
    "d['load-2h'] = float('NaN') #previous hr\n",
    "d['load-1d-2h'] = float('NaN') #previous hr\n",
    "d['load-1w-2h'] = float('NaN') #previous hr\n",
    "\n",
    "#24 hour shift = 24 shift in index.\n",
    "\n",
    "d.loc[24:d.shape[0],'load-1d'] = list(d['load'].iloc[0:(d.shape[0]-24)])\n",
    "d.loc[(24*2):d.shape[0],'load-2d'] = list(d['load'].iloc[0:(d.shape[0]-(24*2))])\n",
    "d.loc[(24*3):d.shape[0],'load-3d'] = list(d['load'].iloc[0:(d.shape[0]-(24*3))])\n",
    "d.loc[(24*4):d.shape[0],'load-4d'] = list(d['load'].iloc[0:(d.shape[0]-(24*4))])\n",
    "d.loc[(24*5):d.shape[0],'load-5d'] = list(d['load'].iloc[0:(d.shape[0]-(24*5))])\n",
    "d.loc[(24*6):d.shape[0],'load-6d'] = list(d['load'].iloc[0:(d.shape[0]-(24*6))])\n",
    "d.loc[(24*7):d.shape[0],'load-1w'] = list(d['load'].iloc[0:(d.shape[0]-(24*7))])\n",
    "\n",
    "d.loc[1:d.shape[0],'load-1h'] = list(d['load'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[25:d.shape[0],'load-1d-1h'] = list(d['load'].iloc[0:(d.shape[0]-25)])\n",
    "d.loc[(24*7+1):d.shape[0],'load-1w-1h'] = list(d['load'].iloc[0:(d.shape[0]-(24*7+1))])\n",
    "d.loc[2:d.shape[0],'load-2h'] = list(d['load'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[26:d.shape[0],'load-1d-2h'] = list(d['load'].iloc[0:(d.shape[0]-26)])\n",
    "d.loc[(24*7+2):d.shape[0],'load-1w-2h'] = list(d['load'].iloc[0:(d.shape[0]-(24*7+2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8760, 40), (8760, 40))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['tsLocal'].map(lambda x: x.year==2013)].shape,d[d['tsLocal'].map(lambda x: x.year==2014)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adjust date correctly\n",
    "datetime.date(2013,1,1).weekday(),datetime.date(2014,1,1).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tsLocal</th>\n",
       "      <th>cldCvr</th>\n",
       "      <th>dewPt</th>\n",
       "      <th>feelsLike</th>\n",
       "      <th>precip</th>\n",
       "      <th>relHum</th>\n",
       "      <th>sfcPres</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>spcHum</th>\n",
       "      <th>temp</th>\n",
       "      <th>...</th>\n",
       "      <th>load-4d</th>\n",
       "      <th>load-5d</th>\n",
       "      <th>load-6d</th>\n",
       "      <th>load-1w</th>\n",
       "      <th>load-1h</th>\n",
       "      <th>load-1d-1h</th>\n",
       "      <th>load-1w-1h</th>\n",
       "      <th>load-2h</th>\n",
       "      <th>load-1d-2h</th>\n",
       "      <th>load-1w-2h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8760</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td> 15</td>\n",
       "      <td> 12.5</td>\n",
       "      <td> 19.3</td>\n",
       "      <td> 0</td>\n",
       "      <td> 56.2</td>\n",
       "      <td> 1022.8</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1.64</td>\n",
       "      <td> 26</td>\n",
       "      <td>...</td>\n",
       "      <td> 90592.079</td>\n",
       "      <td> 94092.666</td>\n",
       "      <td> 90176.256</td>\n",
       "      <td> 96348.839</td>\n",
       "      <td> 97502.767</td>\n",
       "      <td> 99773.947</td>\n",
       "      <td> 99644.327</td>\n",
       "      <td> 100683.011</td>\n",
       "      <td> 104914.285</td>\n",
       "      <td> 101478.199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tsLocal  cldCvr  dewPt  feelsLike  precip  relHum  sfcPres  snowfall  \\\n",
       "8760 2014-01-01      15   12.5       19.3       0    56.2   1022.8         0   \n",
       "\n",
       "      spcHum  temp     ...        load-4d    load-5d    load-6d    load-1w  \\\n",
       "8760    1.64    26     ...      90592.079  94092.666  90176.256  96348.839   \n",
       "\n",
       "        load-1h  load-1d-1h  load-1w-1h     load-2h  load-1d-2h  load-1w-2h  \n",
       "8760  97502.767   99773.947   99644.327  100683.011  104914.285  101478.199  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d.tsLocal.map(lambda x: x.date() == datetime.date(2014,1,1))][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['load-1y'] = float('NaN')\n",
    "#do one year shift. With weekday correction\n",
    "d.loc[(8759):d.shape[0],'load-1y'] = list(d['load'].iloc[24:(d.shape[0]-(8759-24))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['load_ref'] = d['load-1d']*((d['weekday']!=0)&(d['weekday']!=5))+\\\n",
    "                    d['load-1w']*(((d['weekday']==0)|(d['weekday']==5)))\n",
    "                    #0.5*(d['load-1w']+d['load-1y'])*(((d['weekday']==0)|(d['weekday']==5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extra modification to make it smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ref... not use anymore \n",
    "#d['load_ref+1'] = float('NaN')\n",
    "#d['load_ref-1'] = float('NaN')\n",
    "#d.loc[1:d.shape[0],'load_ref-1'] = list(d['load_ref'].iloc[0:(d.shape[0]-1)])\n",
    "#d.loc[0:(d.shape[0]-2),'load_ref+1'] = list(d['load_ref'].iloc[1:(d.shape[0])])\n",
    "#d['load_ref'] = (d['load_ref']+d['load_ref-1']+d['load_ref+1'])/3.0\n",
    "\n",
    "#for day\n",
    "d['load-1d+1'] = float('NaN')\n",
    "d['load-1d-1'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'load-1d-1'] = list(d['load-1d'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[0:(d.shape[0]-2),'load-1d+1'] = list(d['load-1d'].iloc[1:(d.shape[0])])\n",
    "d['load-1d_smooth'] = (d['load-1d']+d['load-1d-1']+d['load-1d+1'])/3.0\n",
    "\n",
    "#for week\n",
    "d['load-1w+1'] = float('NaN')\n",
    "d['load-1w-1'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'load-1w-1'] = list(d['load-1w'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[0:(d.shape[0]-2),'load-1w+1'] = list(d['load-1w'].iloc[1:(d.shape[0])])\n",
    "d['load-1w_smooth'] = (d['load-1w']+d['load-1w-1']+d['load-1w+1'])/3.0\n",
    "\n",
    "'''\n",
    "#Define ratio r\n",
    "d['r'] = d['load']/d['load_ref']\n",
    "d['r-1'] = float('NaN')\n",
    "d['r-2'] = float('NaN')\n",
    "d['r-3'] = float('NaN')\n",
    "d['r-4'] = float('NaN')\n",
    "d['r-5'] = float('NaN')\n",
    "d['r-6'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'r-1'] = list(d['r'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[2:d.shape[0],'r-2'] = list(d['r'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[3:d.shape[0],'r-3'] = list(d['r'].iloc[0:(d.shape[0]-3)])\n",
    "d.loc[4:d.shape[0],'r-4'] = list(d['r'].iloc[0:(d.shape[0]-4)])\n",
    "d.loc[5:d.shape[0],'r-5'] = list(d['r'].iloc[0:(d.shape[0]-5)])\n",
    "d.loc[6:d.shape[0],'r-6'] = list(d['r'].iloc[0:(d.shape[0]-6)])\n",
    "'''\n",
    "\n",
    "#Define finer ratio\n",
    "d['rd'] = d['load']/d['load-1d']\n",
    "d['rw'] = d['load']/d['load-1w']\n",
    "#d['ry'] = d['load']/d['load-1y']\n",
    "d['rd-1'] = float('NaN')\n",
    "d['rw-1'] = float('NaN')\n",
    "#d['ry-1'] = float('NaN')\n",
    "d['rd-2'] = float('NaN')\n",
    "d['rw-2'] = float('NaN')\n",
    "#d['ry-2'] = float('NaN')\n",
    "d.loc[1:d.shape[0],'rd-1'] = list(d['rd'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[1:d.shape[0],'rw-1'] = list(d['rw'].iloc[0:(d.shape[0]-1)])\n",
    "#d.loc[1:d.shape[0],'ry-1'] = list(d['ry'].iloc[0:(d.shape[0]-1)])\n",
    "d.loc[2:d.shape[0],'rd-2'] = list(d['rd'].iloc[0:(d.shape[0]-2)])\n",
    "d.loc[2:d.shape[0],'rw-2'] = list(d['rw'].iloc[0:(d.shape[0]-2)])\n",
    "#d.loc[2:d.shape[0],'ry-2'] = list(d['ry'].iloc[0:(d.shape[0]-2)])\n",
    "\n",
    "#feed this back to data\n",
    "Data = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1-hr rolling prediction: Naive model, load and 'r' investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Data\n",
    "#Truly random\n",
    "#l = np.random.permutation(d.shape[0])\n",
    "#d = d.iloc[l].reset_index(drop=True)\n",
    "#offset = int(d.shape[0] * 0.8)\n",
    "\n",
    "#Assign specific period\n",
    "StartDate = datetime.date(2014,10,1)\n",
    "offset = d[d.tsLocal.map(lambda x: x.date()) <StartDate].shape[0]\n",
    "\n",
    "Data_train = d[:offset]\n",
    "Data_test = d[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120847.897, 86464.576358858176)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Data_test['load']), np.mean(Data_test['load'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Reference: Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = Data_train\n",
    "d_test = Data_test\n",
    "feature_names = ['rd-1','rw-1']\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load-1w','load-1d']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load-1w','load-1d']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[606.91442847187125, 5133.1581509796379]\n"
     ]
    }
   ],
   "source": [
    "# with reference with previous day + week\n",
    "d_test['load_predict']=float('NaN')\n",
    "d_test['load_predict']=0.5*(d_test['rd-1']*d_test['load-1d']+d_test['rw-1']*d_test['load-1w'])\n",
    "\n",
    "l_0 = d_test['load']-d_test['load_predict']\n",
    "[mean_persistence,max_persistence] = [np.mean(np.absolute(l_0)),np.max(np.absolute(l_0))]\n",
    "\n",
    "print [mean_persistence,max_persistence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex model: GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Boosting Regression\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.1, 'loss': 'lad'}\n",
    "mod = ensemble.GradientBoostingRegressor(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399.31345460907744, 4660.5417515401787]\n"
     ]
    }
   ],
   "source": [
    "#best predictors set for CA and NJ are different\n",
    "'''\n",
    "if zipcode == '92562':\n",
    "    feature_names = ['rw-1','rd-1','d','h']\n",
    "\n",
    "    d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "    d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "    # run individual models for each day of week\n",
    "    d_test['load_predict']= float('NaN')\n",
    "\n",
    "    feature_names = ['rw-1','d','h']\n",
    "    X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "    X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "    mod.fit(X_train, y_train)\n",
    "    d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "    feature_names = ['rd-1','d','h']\n",
    "    X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "    X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "    mod.fit(X_train, y_train)\n",
    "    d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "\n",
    "    d_test['load_predict'] = 0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "            \n",
    "if zipcode == '08641':\n",
    "'''\n",
    "feature_names = ['rw-1','rd-1','rw-2','rd-2','d','h']\n",
    "\n",
    "d_train = Data_train.loc[:,['tsLocal','load','load_ref','load-1w','load','rw','rd']+feature_names].dropna()\n",
    "d_test = Data_test.loc[:,['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "\n",
    "# run individual models for each day of week\n",
    "d_test['load_predict']= float('NaN')\n",
    "\n",
    "feature_names = ['rw-1','rw-2','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rw']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rw']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_w'] = (mod.predict(X_test))*d_test.loc[:,'load-1w']\n",
    "\n",
    "feature_names = ['rd-1','rd-2','d','h']\n",
    "X_train, y_train = d_train.loc[:,feature_names], d_train.loc[:,'rd']\n",
    "X_test, y_test = d_test.loc[:,feature_names], d_test.loc[:,'rd']\n",
    "mod.fit(X_train, y_train)\n",
    "d_test.loc[:,'load_predict_d'] = (mod.predict(X_test))*d_test.loc[:,'load-1d']\n",
    "\n",
    "d_test['load_predict'] = 0.5*(d_test['load_predict_d']+d_test['load_predict_w'])\n",
    "\n",
    "l = d_test['load']-d_test['load_predict']\n",
    "[mean_batch,max_batch] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "\n",
    "print [mean_batch,max_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Scheme 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data['load_predict'] = float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DateOfInterest = StartDate \n",
    "\n",
    "while DateOfInterest < EndScopeTime.date():\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rw-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rw-1','rw-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_w'] = (mod.predict(X_test))*d.loc[:,'load-1w']\n",
    "    Data.loc[d.index,'load_predict_w'] =d['load_predict_w']\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rd-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rd-1','rd-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_d'] = (mod.predict(X_test))*d.loc[:,'load-1d']\n",
    "    Data.loc[d.index,'load_predict_d'] =d['load_predict_d']\n",
    "\n",
    "    DateOfInterest = DateOfInterest + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[396.7886953902547, 4340.7760616463493]\n"
     ]
    }
   ],
   "source": [
    "l = Data.loc[offset:,'load']-0.5*(Data.loc[offset:,'load_predict_w']+Data.loc[offset:,'load_predict_d'])\n",
    "[mean_scheme1,max_scheme1] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "print [mean_scheme1,max_scheme1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Scheme 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DateOfInterest = StartDate \n",
    "\n",
    "while DateOfInterest < EndScopeTime.date():\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rw-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rw-1','rw-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    d = d[d['tsLocal'].map(lambda x: x.date()>=(DateOfInterest-datetime.timedelta(days=30*6)))]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_w'] = (mod.predict(X_test))*d.loc[:,'load-1w']\n",
    "    Data.loc[d.index,'load_predict_w'] =d['load_predict_w']\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rd-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rd-1','rd-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    d = d[d['tsLocal'].map(lambda x: x.date()>=(DateOfInterest-datetime.timedelta(days=30*6)))]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_d'] = (mod.predict(X_test))*d.loc[:,'load-1d']\n",
    "    Data.loc[d.index,'load_predict_d'] =d['load_predict_d']\n",
    "\n",
    "    DateOfInterest = DateOfInterest + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[525.32754905952584, 7477.3204395205103]\n"
     ]
    }
   ],
   "source": [
    "l = Data.loc[offset:,'load']-0.5*(Data.loc[offset:,'load_predict_w']+Data.loc[offset:,'load_predict_d'])\n",
    "[mean_scheme2,max_scheme2] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "print [mean_scheme2,max_scheme2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DateOfInterest = StartDate \n",
    "\n",
    "while DateOfInterest < EndScopeTime.date():\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rw-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rw-1','rw-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    d = d[d['tsLocal'].map(lambda x: x.date()>=(DateOfInterest-datetime.timedelta(days=30*3)))]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_w'] = (mod.predict(X_test))*d.loc[:,'load-1w']\n",
    "    Data.loc[d.index,'load_predict_w'] =d['load_predict_w']\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rd-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rd-1','rd-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    d = d[d['tsLocal'].map(lambda x: x.date()>=(DateOfInterest-datetime.timedelta(days=30*3)))]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_d'] = (mod.predict(X_test))*d.loc[:,'load-1d']\n",
    "    Data.loc[d.index,'load_predict_d'] =d['load_predict_d']\n",
    "\n",
    "    DateOfInterest = DateOfInterest + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[569.05664780764266, 7265.321675913714]\n"
     ]
    }
   ],
   "source": [
    "l = Data.loc[offset:,'load']-0.5*(Data.loc[offset:,'load_predict_w']+Data.loc[offset:,'load_predict_d'])\n",
    "[mean_scheme3,max_scheme3] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "print [mean_scheme3,max_scheme3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DateOfInterest = StartDate \n",
    "\n",
    "while DateOfInterest < EndScopeTime.date():\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rw-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rw-1','rw-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    d = d[d['tsLocal'].map(lambda x: x.date()>=(DateOfInterest-datetime.timedelta(days=30*1)))]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rw']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_w'] = (mod.predict(X_test))*d.loc[:,'load-1w']\n",
    "    Data.loc[d.index,'load_predict_w'] =d['load_predict_w']\n",
    "\n",
    "    '''\n",
    "    if zipcode == '92562':\n",
    "        feature_names = ['rd-1','d','h']\n",
    "    if zipcode == '08641':\n",
    "    '''\n",
    "    feature_names = ['rd-1','rd-2','d','h']\n",
    "    #d_imm = Data.loc[Data[Data.tsLocal.map(lambda x: ((x.weekday()==DateOfInterest.weekday())))].index,\n",
    "    d_imm = Data.loc[:,\n",
    "              ['tsLocal','load','load_ref','load-1w','load-1d','rw','rd']+feature_names].dropna()\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()<DateOfInterest)]\n",
    "    d = d[d['tsLocal'].map(lambda x: x.date()>=(DateOfInterest-datetime.timedelta(days=30*1)))]\n",
    "    X_train, y_train = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "    d = d_imm[d_imm['tsLocal'].map(lambda x: x.date()==DateOfInterest)]\n",
    "    X_test, y_test = d.loc[:,feature_names], d.loc[:,'rd']\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    d.loc[:,'load_predict_d'] = (mod.predict(X_test))*d.loc[:,'load-1d']\n",
    "    Data.loc[d.index,'load_predict_d'] =d['load_predict_d']\n",
    "\n",
    "    DateOfInterest = DateOfInterest + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[569.05664780764266, 7374.7643997696141]\n"
     ]
    }
   ],
   "source": [
    "l = Data.loc[offset:,'load']-0.5*(Data.loc[offset:,'load_predict_w']+Data.loc[offset:,'load_predict_d'])\n",
    "[mean_scheme4,max_scheme4] = [np.mean(np.absolute(l)),np.max(np.absolute(l))]\n",
    "print [mean_scheme3,max_scheme4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
